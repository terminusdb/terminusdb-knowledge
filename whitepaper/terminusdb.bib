@article{Codd:1970:RMD:362384.362685,
 author = {Codd, E. F.},
 title = {A Relational Model of Data for Large Shared Data Banks},
 journal = {Commun. ACM},
 issue_date = {June 1970},
 volume = {13},
 number = {6},
 month = jun,
 year = {1970},
 issn = {0001-0782},
 pages = {377--387},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/362384.362685},
 doi = {10.1145/362384.362685},
 acmid = {362685},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {composition, consistency, data bank, data base, data integrity, data organization, data structure, derivability, hierarchies of data, join, networks of data, predicate calculus, redundancy, relations, retrieval language, security},
}

@InProceedings{10.1007/978-3-642-30284-8_36,
author="Mart{\'i}nez-Prieto, Miguel A.
and Arias Gallego, Mario
and Fern{\'a}ndez, Javier D.",
editor="Simperl, Elena
and Cimiano, Philipp
and Polleres, Axel
and Corcho, Oscar
and Presutti, Valentina",
title="Exchange and Consumption of Huge RDF Data",
booktitle="The Semantic Web: Research and Applications",
year="2012",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="437--452",
abstract="Huge RDF datasets are currently exchanged on textual RDF formats, hence consumers need to post-process them using RDF stores for local consumption, such as indexing and SPARQL query. This results in a painful task requiring a great effort in terms of time and computational resources. A first approach to lightweight data exchange is a compact (binary) RDF serialization format called HDT. In this paper, we show how to enhance the exchanged HDT with additional structures to support some basic forms of SPARQL query resolution without the need of ''unpacking'' the data. Experiments show that i) with an exchanging efficiency that outperforms universal compression, ii) post-processing now becomes a fast process which iii) provides competitive query performance at consumption.",
isbn="978-3-642-30284-8"
}

@INPROCEEDINGS{65147,
author={G. E. {Kaiser} and D. E. {Perry} and W. M. {Schell}},
booktitle={[1989] Proceedings of the Thirteenth Annual International Computer Software Applications Conference},
title={Infuse: fusing integration test management with change management},
year={1989},
volume={},
number={},
pages={552-558},
keywords={program testing;programming environments;software engineering;software maintenance;software evolution;unit testing;Infuse;integration test management;change management;software development environment;change coordination;large-scale software projects;integrate strongly connected modules;weakly connected sets;hierarchy;singletons;clusters;interdependent modules;merging;change set;baseline;dynamic consistency;leaves;integration testing;intermediate clusters;acceptance testing;root;test harnesses;regression test suites;C;Software testing;Automatic testing;Programming;Software maintenance;Modular construction;Environmental management;Project management;Software development management;System testing;Research and development management},
doi={10.1109/CMPSAC.1989.65147},
ISSN={null},
month={9},}

@article{LAUKKANEN201755,
title = "Problems, causes and solutions when adopting continuous delivery—A systematic literature review",
journal = "Information and Software Technology",
volume = "82",
pages = "55 - 79",
year = "2017",
issn = "0950-5849",
doi = {https://doi.org/10.1016/j.infsof.2016.10.001},
url = {http://www.sciencedirect.com/science/article/pii/S0950584916302324},
author = "Eero Laukkanen and Juha Itkonen and Casper Lassenius",
keywords = "Continuous integration, Continuous delivery, Continuous deployment, Systematic literature review",
abstract = "Context: Continuous delivery is a software development discipline in which software is always kept releasable. The literature contains instructions on how to adopt continuous delivery, but the adoption has been challenging in practice. Objective: In this study, a systematic literature review is conducted to survey the faced problems when adopting continuous delivery. In addition, we identify causes for and solutions to the problems. Method: By searching five major bibliographic databases, we identified 293 articles related to continuous delivery. We selected 30 of them for further analysis based on them containing empirical evidence of adoption of continuous delivery, and focus on practice instead of only tooling. We analyzed the selected articles qualitatively and extracted problems, causes and solutions. The problems and solutions were thematically synthesized into seven themes: build design, system design, integration, testing, release, human and organizational and resource. Results: We identified a total of 40 problems, 28 causal relationships and 29 solutions related to adoption of continuous delivery. Testing and integration problems were reported most often, while the most critical reported problems were related to testing and system design. Causally, system design and testing were most connected to other themes. Solutions in the system design, resource and human and organizational themes had the most significant impact on the other themes. The system design and build design themes had the least reported solutions. Conclusions: When adopting continuous delivery, problems related to system design are common, critical and little studied. The found problems, causes and solutions can be used to solve problems when adopting continuous delivery in practice."
}

@phdthesis{Jacobson:1988:SSD:915547,
 author = {Jacobson, Guy Joseph},
 title = {Succinct Static Data Structures},
 year = {1988},
 note = {AAI8918056},
 publisher = {Carnegie Mellon University},
 address = {Pittsburgh, PA, USA},
}

@article{doi:10.1002/spe.2198,
author = {Gog, Simon and Petri, Matthias},
title = {Optimized succinct data structures for massive data},
journal = {Software: Practice and Experience},

volume = {44},
number = {11},
pages = {1287-1314},
keywords = {succinct data structures, binary sequences, FM-index, algorithm engineering, massive data sets, rank, select, SSE, hugepages},
doi = {10.1002/spe.2198},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.2198},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/spe.2198},
abstract = {SUMMARYSuccinct data structures provide the same functionality as their corresponding traditional data structure in compact space. We improve on functions rank and select, which are the basic building blocks of FM-indexes and other succinct data structures. First, we present a cache-optimal, uncompressed bitvector representation that outperforms all existing approaches. Next, we improve, in both space and time, on a recent result by Navarro and Providel on compressed bitvectors. Last, we show techniques to perform rank and select on 64-bit words that are up to three times faster than existing methods. In our experimental evaluation, we first show how our improvements affect cache and runtime performance of both operations on data sets larger than commonly used in the evaluation of succinct data structures. Our experiments show that our improvements to these basic operations significantly improve the runtime performance and compression effectiveness of FM-indexes on small and large data sets. To our knowledge, our improvements result in FM-indexes that are either smaller or faster than all current state of the art implementations. Copyright © 2013 John Wiley \& Sons, Ltd.},
year = {2014}
}

@article{MARTINEZPRIETO201673,
title = "Practical compressed string dictionaries",
journal = "Information Systems",
volume = "56",
pages = "73 - 108",
year = "2016",
issn = "0306-4379",
doi = {https://doi.org/10.1016/j.is.2015.08.008},
url = {http://www.sciencedirect.com/science/article/pii/S0306437915001672},
author = "Miguel A. Martínez-Prieto and Nieves Brisaboa and Rodrigo Cánovas and Francisco Claude and Gonzalo Navarro",
keywords = "Compressed string dictionaries, Text processing, Text databases, Compressed data structures",
% abstract = {The need to store and query a set of strings – a string dictionary – arises in many kinds of applications. While classically these string dictionaries have accounted for a small share of the total space budget (e.g., in Natural Language Processing or when indexing text collections), recent applications in Web engines, Semantic Web (RDF) graphs, Bioinformatics, and many others handle very large string dictionaries, whose size is a significant fraction of the whole data. In these cases, string dictionary management is a scalability issue by itself. This paper focuses on the problem of managing large static string dictionaries in compressed main memory space. We revisit classical solutions for string dictionaries like hashing, tries, and front-coding, and improve them by using compression techniques. We also introduce some novel string dictionary representations built on top of recent advances in succinct data structures and full-text indexes. All these structures are empirically compared on a heterogeneous testbed formed by real-world string dictionaries. We show that the compressed representations may use as little as 5% of the original dictionary size, while supporting lookup operations within a few microseconds. These numbers outperform the state-of-the-art space/time tradeoffs in many cases. Furthermore, we enhance some representations to provide prefix- and substring-based searches, which also perform competitively. The results show that compressed string dictionaries are a useful building block for various data-intensive applications in different domains.}
}

@article{DBLP:journals/semweb/FeeneyMB18,
  author    = {Kevin Chekov Feeney and
               Gavin Mendel{-}Gleason and
               Rob Brennan},
  title     = {Linked data schemata: Fixing unsound foundations},
  journal   = {Semantic Web},
  volume    = {9},
  number    = {1},
  pages     = {53--75},
  year      = {2018},
  url       = {https://doi.org/10.3233/SW-170271},
  doi       = {10.3233/SW-170271},
  timestamp = {Wed, 13 Dec 2017 15:13:19 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/semweb/FeeneyMB18},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Ferragina:2005:ICT:1082036.1082039,
 author = {Ferragina, Paolo and Manzini, Giovanni},
 title = {Indexing Compressed Text},
 journal = {J. ACM},
 issue_date = {July 2005},
 volume = {52},
 number = {4},
 month = jul,
 year = {2005},
 issn = {0004-5411},
 pages = {552--581},
 numpages = {30},
 url = {http://doi.acm.org/10.1145/1082036.1082039},
 doi = {10.1145/1082036.1082039},
 acmid = {1082039},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Burrows--Wheeler transform, Lempel--Ziv compressor, full-text indexing, indexing data structure, pattern searching, suffix array, suffix tree, text compression},
}

@book{Blandy:2015:RPL:3019371,
 author = {Blandy, Jim},
 title = {The Rust Programming Language: Fast, Safe, and Beautiful},
 year = {2015},
 isbn = {9781491925447},
 publisher = {O'Reilly Media, Inc.},
}

@inproceedings{Mohan:1992:EFM:130283.130306,
 author = {Mohan, C. and Pirahesh, Hamid and Lorie, Raymond},
 title = {Efficient and Flexible Methods for Transient Versioning of Records to Avoid Locking by Read-only Transactions},
 booktitle = {Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data},
 series = {SIGMOD '92},
 year = {1992},
 isbn = {0-89791-521-6},
 location = {San Diego, California, USA},
 pages = {124--133},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/130283.130306},
 doi = {10.1145/130283.130306},
 acmid = {130306},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@article{Sadoghi:2014:RDL:2733004.2733006,
 author = {Sadoghi, Mohammad and Canim, Mustafa and Bhattacharjee, Bishwaranjan and Nagel, Fabian and Ross, Kenneth A.},
 title = {Reducing Database Locking Contention Through Multi-version Concurrency},
 journal = {Proc. VLDB Endow.},
 issue_date = {August 2014},
 volume = {7},
 number = {13},
 month = aug,
 year = {2014},
 issn = {2150-8097},
 pages = {1331--1342},
 numpages = {12},
 url = {http://dx.doi.org/10.14778/2733004.2733006},
 doi = {10.14778/2733004.2733006},
 acmid = {2733006},
 publisher = {VLDB Endowment},
}

@techreport{gleason_feeney2018,
title = "Evolving Meaningful Value From Enterprise Data",
year = "2018",
institution = "TerminusDB",
note = {URL: \url{https://www.datachemist.com/images/uploads/general/Evolving_MeaningfulValuefromEnterpriseD
ata.pdf}},
author = "Mendel-Gleason, Gavin and Feeney, Kevin and O'Donoghue, Jim and {\L}obocka, Sabina and Zejer, Piotr and B{\l}{\k e}dski, Piotr and Dirschl, Christian",
}

@techreport{gleason2018,
title = "Machine Checkable Formalisation of OWL Semantics",
institution = "TerminusDB",
year = "2018",
note = {URL: \url{http://terminusdb.com/t/docs/OWL-formalisation.pdf}},
author = "Mendel-Gleason, Gavin",
}
